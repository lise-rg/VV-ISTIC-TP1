# Practical Session #1: Introduction

1. Find in news sources a general public article reporting the discovery of a software bug. Describe the bug. If possible, say whether the bug is local or global and describe the failure that manifested its presence. Explain the repercussions of the bug for clients/consumers and the company or entity behind the faulty program. Speculate whether, in your opinion, testing the right scenario would have helped to discover the fault.

2. Apache Commons projects are known for the quality of their code and development practices. They use dedicated issue-tracking systems to discuss and follow the evolution of bugs and new features. The following link https://issues.apache.org/jira/projects/COLLECTIONS/issues/COLLECTIONS-794?filter=doneissues points to the issues considered as solved for the Apache Commons Collections project. Among those issues find one that corresponds to a bug that has been solved. Classify the bug as local or global. Explain the bug and the solution. Did the contributors of the project add new tests to ensure that the bug is detected if it reappears in the future?

While looking through the Apache Commons Jira, I stumbled upon a minor bug. The function `SetUniqueList.createSetBasedOnList`, which used to add elements from a provided `list` parameter to the return value, no longer did so, despite no alterations in the documentation. After investigating, they found that a call to an `addAll` function was accidentally deleted.
After analyzing the intentions behind the modifications, they realized that it was indeed a mistake. The issue was fixed by reinstating the missing call and introducing a new test to verify that functionality. This was a simple incident that could have been avoided with a proper non-regression test. 

3. Netflix is famous, among other things we love, for the popularization of *Chaos Engineering*, a fault-tolerance verification technique. The company has implemented protocols to test its entire system in production by simulating faults such as a server shutdown. During these experiments, they evaluate the system's capabilities of delivering content under different conditions. The technique was described in [a paper](https://arxiv.org/ftp/arxiv/papers/1702/1702.05843.pdf) published in 2016. Read the paper and briefly explain what are the concrete experiments they perform, what are the requirements for these experiments, what are the variables they observe and what are the main results they obtained. Is Netflix the only company performing these experiments? Speculate how these experiments could be carried out in other organizations in terms of the kind of experiment that could be performed and the system variables to observe during the experiments.

Chaos Engineering involves conducting experiments on a distributed system to build confidence in its ability to endure turbulent conditions in production. Its purpose is to ensure software services can withstand failures of individual instances.
Outlined in this paper are some instances of input used by Netflix in these experiments, such as terminating virtual machine instances, injecting latency into requests between services, failing requests between services, failing an internal service, and making an entire Amazon region unavailable.

Counterintuitively, Chaos Engineering is a highly formalized process. Designing an experiment requires specifying: hypotheses, independent variables (real-world events), dependent variables (experiments in production), and context.
Although replicating past system outages is a valid input method, any input that could potentially disrupt the steady-state behavior of the system is a good candidate. Those experiments are only conducted during working hours for the engineer to respond quickly if a service fails due to an instance termination.

To monitor changes, Netflix observes the rate at which users start streaming videos each second, known as SPS (stream starts per second). This metric, along with others like new account signups per second, characterizes the steady-state behavior of the system. These metrics are usually checked against standard ranges to identify variations. 
The ultimate goal of Chaos Engineering is to employ automation to maintain confidence in results over time.
Despite this, the preference is to conduct experiments in a production environment whenever possible.

Netflix is not the only company performing these experiments, many large tech organizations like Amazon, Google, Microsoft, and Facebook are applying similar techniques to test the resilience of their system.
Amazon, for instance, conducts controlled experiments such as shutting down data centers to ensure that their services can handle unexpected disruptions without causing outages. Google on the other hand, often introduces network latency, server failures, and other inputs to assess the resilience of their services. I believe that both of them would use data similar to the ones used by Netflix to monitor changes, such as the number of research, number of purchases, or number of accounts created per second.

4. [WebAssembly](https://webassembly.org/) has become the fourth official language supported by web browsers. The language was born from a joint effort of the major players on the Web. Its creators presented their design decisions and the formal specification in [a scientific paper](https://people.mpi-sws.org/~rossberg/papers/Haas,%20Rossberg,%20Schuff,%20Titzer,%20Gohman,%20Wagner,%20Zakai,%20Bastien,%20Holman%20-%20Bringing%20the%20Web%20up%20to%20Speed%20with%20WebAssembly.pdf) published in 2018. The goal of the language is to be a low-level, safe, and portable compilation target for the Web and other embedding environments. The authors say that it is the first industrial-strength language designed with formal semantics from the start. This evidences the feasibility of constructive approaches in this area. Read the paper and explain what are the main advantages of having a formal specification for WebAssembly. In your opinion, does this mean that WebAssembly implementations should not be tested?

Having a formal specification for WebAssembly offers several advantages. First of all, it provides a clear and precise description of the language. This helps achieve consistency and leads to a notably clean design.

The structured control flow of WebAssembly makes validation and compilation fast and simple. It becomes easier to verify and validate the WebAssembly implementations.

It also enables interoperability between different implementations of WebAssembly. It ensures that code written in WebAssembly can run consistently across various platforms and browsers, regardless of the underlying hardware or software differences.

Overall, a formal specification for WebAssembly enhances the reliability, portability, and compatibility of the language.
However, I believe that all implementations should still be tested to ensure that the code correctly adheres to the specification and produces the expected results. Testing also helps identify any potential bugs, inconsistencies, or performance issues in the implementation, which may not be apparent from the formal specification alone.

5.  Shortly after the appearance of WebAssembly another paper proposed a mechanized specification of the language using Isabelle. The paper can be consulted here: https://www.cl.cam.ac.uk/~caw77/papers/mechanising-and-verifying-the-webassembly-specification.pdf. This mechanized specification complements the first formalization attempt from the paper. According to the author of this second paper, what are the main advantages of the mechanized specification? Did it help improve the original formal specification of the language? What other artifacts were derived from this mechanized specification? How did the author verify the specification? Does this new specification remove the need for testing?

## Answers
